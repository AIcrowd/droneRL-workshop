{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Colab Setup\n",
    "---\n",
    "\n",
    "Make sure to select GPU in Runtime > Change runtime type > Hardware accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title << Run this to check your runtime is correct {display-mode: \"form\"}\n",
    "!nvidia-smi | grep -q 'failed' && echo \"STOP! You are using a runtime without a GPU. Change the runtime type before going further!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title << Setup Google Colab by running this cell {display-mode: \"form\"}\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Clone GitHub repository\n",
    "    !git clone --single-branch --branch evaluation_setup https://github.com/pacm/rl-workshop.git\n",
    "        \n",
    "    # Copy files required to run the code\n",
    "    !cp -r \"rl-workshop/agents\" \"rl-workshop/env\" \"rl-workshop/rl_helpers\" .\n",
    "    \n",
    "    # Install packages via pip\n",
    "    !pip install -r \"rl-workshop/colab-requirements.txt\"\n",
    "    \n",
    "    # Restart Runtime\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prioritized Experience Replay\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from agents.dqn import DQNAgent, ConvQNetworkFactory, ConvQNetwork\n",
    "from agents.curiosity import CuriosityDQNAgent\n",
    "from agents.random import RandomAgent\n",
    "from agents.logging import TensorBoardLogger, NoLogger\n",
    "from agents.per import PERAgent\n",
    "from env.env import WindowedGridView, DeliveryDrones\n",
    "from rl_helpers.rl_helpers import MultiAgentTrainer, test_agents, plot_cumulative_rewards, plot_rolling_rewards, render_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WindowedGridView(DeliveryDrones(), radius=3)\n",
    "\n",
    "# These are the default parameters used for evaluation\n",
    "env.env_params.update({\n",
    "    'charge': 20,\n",
    "    'charge_reward': -0.1,\n",
    "    'crash_reward': -1,\n",
    "    'delivery_reward': 1,\n",
    "    'discharge': 10,\n",
    "    'drone_density': 0.05,\n",
    "    'dropzones_factor': 2,\n",
    "    'n_drones': 10,\n",
    "    'packets_factor': 3,\n",
    "    'pickup_reward': 0,\n",
    "    'rgb_render_rescale': 1.0,\n",
    "    'skyscrapers_factor': 3,\n",
    "    'stations_factor': 2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 DQN agents\n",
    "dqn_agent_1 = DQNAgent(\n",
    "    env, ConvQNetworkFactory(env, conv_layers=[\n",
    "        {'out_channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1},\n",
    "        {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "        {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "        {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    ], dense_layers=[256]),\n",
    "    gamma=0.95, epsilon_start=1, epsilon_decay=0.99, epsilon_end=0.01, memory_size=10000, batch_size=64, \n",
    "    target_update_interval=500)\n",
    "\n",
    "dqn_agent_2 = DQNAgent(\n",
    "    env, ConvQNetworkFactory(env, conv_layers=[\n",
    "        {'out_channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1},\n",
    "        {'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1}\n",
    "    ], dense_layers=[64, 64]),\n",
    "    gamma=0.95, epsilon_start=1, epsilon_decay=0.99, epsilon_end=0.01, memory_size=10000, batch_size=64, \n",
    "    target_update_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1 DQN agent with Prioritized Experience Replay\n",
    "from agents.logging import NoLogger\n",
    "per_logger = NoLogger\n",
    "\n",
    "conv_factory = ConvQNetworkFactory(env, conv_layers=[\n",
    "        {'out_channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1},\n",
    "        {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "        {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "        {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    ], dense_layers=[256])\n",
    "\n",
    "per_agent_1 = PERAgent(env, \n",
    "                       conv_factory, \n",
    "                       gamma=0.95, \n",
    "                       epsilon_start=1.0, \n",
    "                       epsilon_decay=0.99,\n",
    "                       epsilon_end=0.01, \n",
    "                       memory_size=10000, \n",
    "                       batch_size=64, \n",
    "                       target_update_interval=500, \n",
    "                       alpha=0.6, \n",
    "                       beta=0.4, \n",
    "                       logger=per_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment with those parameters\n",
    "env.reset()\n",
    "\n",
    "# Setup random opponents\n",
    "agents = {drone.index: RandomAgent(env) for drone in env.drones}\n",
    "\n",
    "# Add the RL drones\n",
    "agents[0] = dqn_agent_1\n",
    "agents[1] = dqn_agent_2\n",
    "agents[2] = per_agent_1\n",
    "\n",
    "# Create trainer\n",
    "trainer = MultiAgentTrainer(env, agents, reset_agents=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(1000)\n",
    "rewards = plot_rolling_rewards(trainer.rewards_log, drones_labels={0: 'DQN1', 1: 'DQN2', 2: 'PER'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "rewards_log = test_agents(env, agents, n_steps=1000)\n",
    "plot_cumulative_rewards(rewards_log, drones_labels={0: 'DQN1', 1: 'DQN2', 2: 'PER'})\n",
    "\n",
    "# Print final evaluation scores\n",
    "for idx, score in enumerate(np.sum(list(rewards_log.values()), axis=1)):\n",
    "    print(\"Agent {}: {}\".format(idx, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save video if you want\n",
    "path = os.path.join('videos', 'prioritized.mp4')\n",
    "#render_video(env, agents, path, n_steps=60, fps=1, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent (you can ignore the warnings)\n",
    "per_agent_1.save('per-agent-0.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment a bit then submit to AIcrowd :D**\n",
    "\n",
    "> https://www.aicrowd.com/challenges/droneracer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
