{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Colab Setup\n",
    "---\n",
    "\n",
    "Make sure to select GPU in Runtime > Change runtime type > Hardware accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title << Setup Google Colab by running this cell {display-mode: \"form\"}\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Clone GitHub repository\n",
    "    !git clone https://github.com/pacm/rl-workshop.git\n",
    "        \n",
    "    # Copy files required to run the code\n",
    "    !cp -r \"rl-workshop/agents\" \"rl-workshop/env\" \"rl-workshop/helpers\" .\n",
    "    \n",
    "    # Install packages via pip\n",
    "    !pip install -r \"rl-workshop/colab-requirements.txt\"\n",
    "    \n",
    "    # Restart Runtime\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run env/env.py\n",
    "%run helpers/rl_helpers.py\n",
    "%run agents/dqn.py\n",
    "%run agents/qlearning.py\n",
    "%run agents/random.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might want to import other libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro to Q-Learning (compass Q-table)\n",
    "---\n",
    "\n",
    "You can find a Q-learning implementation in `agents/`\n",
    "\n",
    "```\n",
    "agents/\n",
    "├── curiosity.py\n",
    "├── dqn.py\n",
    "├── logging.py\n",
    "├── qlearning.py    <-- Q-learning agent\n",
    "└── random.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment without Skyscrapers + discharge\n",
    "env = CompassQTable(DeliveryDrones())\n",
    "env.env_params.update({'n_drones': 3, 'skyscrapers_factor': 0, 'stations_factor': 0,  'discharge': 0})\n",
    "states = env.reset()\n",
    "\n",
    "print('Observation space:', env.observation_space)\n",
    "print('Initial state:', {drone_index: env.format_state(state) for drone_index, state in states.items()})\n",
    "Image.fromarray(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agents\n",
    "agents = {drone.index: RandomAgent(env) for drone in env.drones}\n",
    "agents[0] = QLearningAgent(\n",
    "    env,\n",
    "    gamma=0.95, # Discount factor\n",
    "    alpha=0.1, # Learning rate\n",
    "    # Exploration rate\n",
    "    epsilon_start=1, epsilon_decay=0.99, epsilon_end=0.01\n",
    ")\n",
    "agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train agents\n",
    "trainer = MultiAgentTrainer(env, agents, reset_agents=True, seed=0)\n",
    "trainer.train(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rolling_rewards(trainer.rewards_log, drones_labels={0: 'Q-learning'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents[0].get_qtable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agents[0].gamma**np.arange(100))\n",
    "plt.title('Discount factor: {}'.format(agents[0].gamma))\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Discount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_log = test_agents(env, agents, n_steps=1000, seed=0)\n",
    "plot_cumulative_rewards(rewards_log, drones_labels={0: 'Q-learning'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('videos', 'ql-compass.mp4')\n",
    "render_video(env, agents, video_path=path, n_steps=120, fps=1, seed=0)\n",
    "ColabVideo(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling Q-learning (compass + lidar Q-table)\n",
    "---\n",
    "\n",
    "Let's see how Q-learning scales to larger observation spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment with skyscrapers but without discharge\n",
    "env = LidarCompassQTable(DeliveryDrones())\n",
    "env.env_params.update({'n_drones': 3, 'skyscrapers_factor': 3, 'stations_factor': 0, 'discharge': 0})\n",
    "states = env.reset()\n",
    "\n",
    "print('Observation space:', env.observation_space)\n",
    "print('Sample state:', {drone_index: env.format_state(state) for drone_index, state in states.items()})\n",
    "Image.fromarray(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agents\n",
    "agents = {drone.index: RandomAgent(env) for drone in env.drones}\n",
    "agents[0] = QLearningAgent(\n",
    "    env,\n",
    "    gamma=0.95, # Discount factor\n",
    "    alpha=0.1, # Learning rate\n",
    "    # Exploration rate\n",
    "    epsilon_start=1, epsilon_decay=0.99, epsilon_end=0.01\n",
    ")\n",
    "agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train agents\n",
    "trainer = MultiAgentTrainer(env, agents, reset_agents=True, seed=0)\n",
    "trainer.train(5000)\n",
    "plot_rolling_rewards(trainer.rewards_log, drones_labels={0: 'Q-learning'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_log = test_agents(env, agents, n_steps=1000, seed=0)\n",
    "plot_cumulative_rewards(rewards_log, drones_labels={0: 'Q-learning'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('videos', 'ql-compass-lidar-1st-try.mp4')\n",
    "render_video(env, agents, video_path=path, n_steps=120, fps=1, seed=0)\n",
    "ColabVideo(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issues with Q-learning\n",
    "---\n",
    "\n",
    "Two issues here\n",
    "\n",
    "* Sparse reward: pickup rate is around 1%\n",
    "* No generalization: need to explore entire space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = agents[0].get_qtable()\n",
    "print('Q-table:', q_table.shape)\n",
    "q_table.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agents[0].epsilons)\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.ylabel('Exploration rate (epsilon)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible solutions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1/2) Sparse rewards: Create an intermediate \"pickup\" reward\n",
    "env.env_params.update({\n",
    "    'n_drones': 3, 'pickup_reward': 0.99, 'delivery_reward': 1,\n",
    "    'skyscrapers_factor': 3, 'stations_factor': 0, 'discharge': 0})\n",
    "states = env.reset()\n",
    "\n",
    "# (2/2) Train longer ..\n",
    "agents[0].epsilon = 1\n",
    "agents[0].epsilon_decay = 0.999\n",
    "\n",
    "set_seed(env, seed=0) # Make things deterministic\n",
    "trainer.train(30000)\n",
    "\n",
    "plot_rolling_rewards(\n",
    "    trainer.rewards_log,\n",
    "    events={'pickup': [0.99], 'delivery': [1], 'crash': [-1]},\n",
    "    drones_labels={0: 'Q-learning'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agents[0].epsilons)\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.ylabel('Exploration rate (epsilon)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_log = test_agents(env, agents, n_steps=1000, seed=0)\n",
    "plot_cumulative_rewards(\n",
    "    rewards_log,\n",
    "    events={'pickup': [0.99], 'delivery': [1], 'crash': [-1]},\n",
    "    drones_labels={0: 'Q-learning'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting issues: try with different seeds\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_log = test_agents(env, agents, n_steps=1000, seed=1)\n",
    "plot_cumulative_rewards(\n",
    "    rewards_log,\n",
    "    events={'pickup': [0.99], 'delivery': [1], 'crash': [-1]},\n",
    "    drones_labels={0: 'Q-learning'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a good seed for your video ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('videos', 'ql-compass-lidar-2nd-try.mp4')\n",
    "render_video(env, agents, video_path=path, n_steps=120, fps=1, seed=1)\n",
    "ColabVideo(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning limitations: discrete Q-table!\n",
    "---\n",
    "\n",
    "Let's try Q-learning with the full environment: skyscrapers + charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = LidarCompassChargeQTable(DeliveryDrones())\n",
    "env.env_params.update({\n",
    "    'n_drones': 3, 'pickup_reward': 0.99, 'delivery_reward': 1,\n",
    "    'discharge': 10, 'charge': 20, 'charge_reward': -0.1  # (default values)\n",
    "})\n",
    "states = env.reset()\n",
    "\n",
    "print('Observation space:', env.observation_space)\n",
    "print('Sample state:', env.format_state(states[0]))\n",
    "Image.fromarray(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agents\n",
    "agents = {drone.index: RandomAgent(env) for drone in env.drones}\n",
    "agents[0] = QLearningAgent(\n",
    "    env, gamma=0.95, alpha=0.1,\n",
    "    epsilon_start=1, epsilon_decay=0.999, epsilon_end=0.01\n",
    ")\n",
    "\n",
    "trainer = MultiAgentTrainer(env, agents, reset_agents=True, seed=0)\n",
    "trainer.train(35000)\n",
    "plot_rolling_rewards(trainer.rewards_log, events={'pickup': [0.99], 'delivery': [1], 'crash': [-1], 'charging': [-0.1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = agents[0].get_qtable()\n",
    "print('Q-table:', q_table.shape)\n",
    "q_table.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to test with different seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_log = test_agents(env, agents, n_steps=1000, seed=0)\n",
    "plot_cumulative_rewards(\n",
    "    rewards_log,\n",
    "    events={'pickup': [0.99], 'delivery': [1], 'crash': [-1], 'charging': [-0.1]},\n",
    "    drones_labels={0: 'Q-learning'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also related to the implementation of our **Trainer class** which **resets the environment only once** at the beginning, before training. Resetting the environment every X steps would help, but won't solve the important limitations with Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('videos', 'ql-compass-lidar-charge.mp4')\n",
    "render_video(env, agents, video_path=path, n_steps=120, fps=1, seed=0)\n",
    "ColabVideo(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Q-learning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = WindowedGridView(DeliveryDrones(), radius=3)\n",
    "\n",
    "# Create agent\n",
    "\"\"\"DQN with dense Q-network\n",
    "my_agent = DQNAgent(\n",
    "    env, DenseQNetworkFactory(env, hidden_layers=[256, 256]),\n",
    "    gamma=0.95, epsilon_start=0.5, epsilon_decay=0.8, epsilon_end=0.01, memory_size=10000, batch_size=64, target_update_interval=5)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"DQN with conv. Q-network\"\"\"\n",
    "my_agent = DQNAgent(\n",
    "    env, ConvQNetworkFactory(env, conv_layers=[\n",
    "        {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "        {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "        {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "        {'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "        {'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "        {'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1}\n",
    "    ], dense_layers=[256]),\n",
    "    gamma=0.95, epsilon_start=1, epsilon_decay=0.99, epsilon_end=0.01, memory_size=10000, batch_size=64, target_update_interval=5)\n",
    "\n",
    "\n",
    "my_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup custom environment parameters for training\n",
    "env.env_params.update({'n_drone': 3, 'pickup_reward': 1, 'discharge': 2, 'rgb_render_rescale': 2.0})\n",
    "\n",
    "# Reset environment with those parameters\n",
    "env.reset()\n",
    "\n",
    "# Setup random opponents\n",
    "agents = {drone.index: RandomAgent(env) for drone in env.drones}\n",
    "agents[0] = my_agent\n",
    "\n",
    "# Create trainer\n",
    "trainer = MultiAgentTrainer(env, agents, reset_agents=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with different grids\n",
    "for _ in range(10):\n",
    "    trainer.train(2000) # Calls env.reset() -> new grid\n",
    "    \n",
    "    # Reset epsilon\n",
    "    if hasattr(my_agent, 'epsilon_start'):\n",
    "        my_agent.epsilon_start *= 0.99\n",
    "        my_agent.epsilon = my_agent.epsilon_start\n",
    "\n",
    "plot_rolling_rewards(trainer.rewards_log, subset=range(0, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Q-learning agent\n",
    "if isinstance(my_agent, RandomAgent):\n",
    "    print('Random agent')\n",
    "    \n",
    "else:\n",
    "    if isinstance(my_agent, QLearningAgent):\n",
    "        # Q-table\n",
    "        print('Q-table:', my_agent.get_qtable().shape)\n",
    "        display(my_agent.get_qtable().sample(10))\n",
    "\n",
    "    # For DQN-agent\n",
    "    elif isinstance(my_agent, DQNAgent):\n",
    "        # Memory replay\n",
    "        my_agent.inspect_memory()\n",
    "\n",
    "        # Q-network\n",
    "        print('Q-network:')\n",
    "        print(my_agent.qnetwork)\n",
    "        print()\n",
    "\n",
    "    # Epsilon decay\n",
    "    plt.plot(my_agent.epsilons)\n",
    "    plt.title('Epsilon decay')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Epsilon')\n",
    "    plt.show()\n",
    "\n",
    "# Test with different seeds\n",
    "for i in range(5):\n",
    "    rewards_log = test_agents(env, agents, n_steps=1000, seed=i)\n",
    "    plot_cumulative_rewards(rewards_log, subset=range(0, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.join('videos', 'video.mp4')\n",
    "render_video(env, agents, path, n_steps=60, fps=1, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "my_agent.save(os.path.join('agents', 'agent.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load agent\n",
    "%run env/env.py\n",
    "%run helpers/rl_helpers.py\n",
    "%run agents/dqn.py\n",
    "%run agents/random.py\n",
    "\n",
    "# Create environment\n",
    "env = WindowedGridView(DeliveryDrones(), radius=3)\n",
    "env.env_params.update({'n_drone': 3, 'pickup_reward': 1, 'discharge': 2, 'rgb_render_rescale': 1.0})\n",
    "env.reset()\n",
    "\n",
    "# Load agent\n",
    "my_agent = DQNAgent(\n",
    "    env, dqn_factory=None, gamma=None, epsilon_start=None, epsilon_decay=None,\n",
    "    epsilon_end=None, memory_size=None, batch_size=None, target_update_interval=None)\n",
    "my_agent.load(os.path.join('agents', 'agent.pt'))\n",
    "\n",
    "# Create opponents\n",
    "agents = {drone.index: RandomAgent(env) for drone in env.drones}\n",
    "agents[0] = my_agent\n",
    "\n",
    "# Test with different seeds\n",
    "for i in range(5):\n",
    "    rewards_log = test_agents(env, agents, n_steps=1000, seed=i)\n",
    "    plot_cumulative_rewards(rewards_log, subset=range(0, 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
