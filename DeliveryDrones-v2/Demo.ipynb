{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%run env.py\n",
    "%run rl-helpers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning and DQN agents\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment:\n",
    "#   (Q-table) CompassQTable, CompassChargeQTable, LidarCompassQTable, LidarCompassChargeQTable\n",
    "#   (Grid)    GlobalGridView, PlayerGridView\n",
    "obs_wrapper = PlayerGridView\n",
    "env = obs_wrapper(DeliveryDrones(n=1))\n",
    "states = env.reset()\n",
    "\n",
    "# Agent\n",
    "\"\"\"Q-learning agent\n",
    "my_agent = QLearningAgent(env, gamma=0.99, alpha=0.1, epsilon_start=1, epsilon_decay=0.99, epsilon_end=0.01)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"DQN with dense Q-network\"\"\"\n",
    "my_agent = DQNAgent(\n",
    "    env, DenseQNetworkFactory(env, hidden_layers=[32]),\n",
    "    gamma=0.99, epsilon_start=1, epsilon_decay=0.99, epsilon_end=0.01, memory_size=10000, batch_size=64, target_update_interval=5)\n",
    "\n",
    "\"\"\"DQN with conv. Q-network\n",
    "my_agent = DQNAgent(\n",
    "    env, ConvQNetworkFactory(env, conv_layers=[\n",
    "        #{'out_channels': 16, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "        #{'out_channels': 16, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "        #{'out_channels': 16, 'kernel_size': 3, 'stride': 1, 'padding': 1}\n",
    "    ], dense_layers=[]),\n",
    "    gamma=0.995, epsilon_start=1, epsilon_decay=0.999, epsilon_end=0.01, memory_size=10000, batch_size=64, target_update_interval=5)\n",
    "\"\"\"\n",
    "\n",
    "# Setup opponents\n",
    "agents = {drone.index: RandomAgent(env) for drone in env.drones}\n",
    "my_drone = env.drones[0]\n",
    "agents[my_drone.index] = my_agent\n",
    "\n",
    "# Train for a few steps, plot results\n",
    "trainer = MultiAgentTrainer(env, agents, seed=0)\n",
    "my_agent.is_greedy = False\n",
    "trainer.train(200)\n",
    "plot_rolling_rewards(trainer.rewards_log, subset=range(1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect agents\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Q-learning agent\n",
    "if isinstance(my_agent, QLearningAgent):\n",
    "    # Q-table\n",
    "    print('Q-table:', my_agent.get_qtable().shape)\n",
    "    display(my_agent.get_qtable().sample(10))\n",
    "\n",
    "# For DQN-agent\n",
    "elif isinstance(my_agent, DQNAgent):\n",
    "    # Memory replay\n",
    "    my_agent.inspect_memory()\n",
    "    \n",
    "    # Q-network\n",
    "    print('Q-network:')\n",
    "    print(my_agent.qnetwork)\n",
    "    print()\n",
    "    \n",
    "# Epsilon decay\n",
    "plt.plot(my_agent.epsilons)\n",
    "plt.title('Epsilon decay')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.show()\n",
    "\n",
    "# Test with different seeds\n",
    "my_agent.is_greedy = True\n",
    "for i in range(10):\n",
    "    rewards_log = test_agents(env, agents, n_steps=1000, seed=i)\n",
    "    plot_cumulative_rewards(rewards_log, subset=range(1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test agents\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "# Make sure our drone behaves greedily\n",
    "my_drone.is_greedy = True\n",
    "\n",
    "# Simulation loop\n",
    "states = env.reset()\n",
    "my_drone = env.drones[0]\n",
    "rewards = None\n",
    "while True:\n",
    "    # Render\n",
    "    clear_output(wait=True)\n",
    "    print(env.render('ainsi'))\n",
    "\n",
    "    # Act\n",
    "    actions = {index: agent.act(states[index]) for index, agent in agents.items()}\n",
    "\n",
    "    # Print last rewards and next actions\n",
    "    print('Drone:', my_drone.index, 'charge: {}%'.format(my_drone.charge))\n",
    "    if hasattr(env, 'format_state'):\n",
    "        print('Current states:', env.format_state(states[my_drone.index]))\n",
    "    if hasattr(env, 'format_action'):\n",
    "        print('Next actions:', env.format_action(actions[my_drone.index]))\n",
    "    if rewards is not None:\n",
    "        print('Last rewards:', rewards[my_drone.index])\n",
    "\n",
    "    # Sleep, step, learn\n",
    "    time.sleep(1)\n",
    "    states, rewards, dones, _ = env.step(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create drones & environment\n",
    "env = PlayerGridView(DeliveryDrones(n=3))\n",
    "states = env.reset()\n",
    "\n",
    "# Run drones\n",
    "for i in tqdm(range(10**6)):\n",
    "    states, rewards, dones, _  = env.step({drone.index: env.action_space.sample() for drone in env.drones})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Development space below\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import gym.spaces as spaces\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run env.py\n",
    "%run rl-helpers.py\n",
    "\n",
    "# Create environment\n",
    "env = PlayerGridView(DeliveryDrones(n=3))\n",
    "states = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_drones, all_drones_positions = env.air.get_objects(Drone)\n",
    "print('Drones:', all_drones, all_drones_positions)\n",
    "\n",
    "all_packets, all_packets_positions = env.ground.get_objects(Packet)\n",
    "print('Packets:', all_packets, all_packets_positions)\n",
    "\n",
    "all_dropzones, all_dropzones_positions = env.ground.get_objects(Dropzone)\n",
    "print('Dropzones:', all_dropzones, all_dropzones_positions)\n",
    "\n",
    "all_stations, all_stations_positions = env.ground.get_objects(Station)\n",
    "print('Stations:', all_stations_positions)\n",
    "\n",
    "print('Drones packets:', [(d, d.packet) for d in all_drones])\n",
    "print('Drones charge:', [(d, d.charge) for d in all_drones])\n",
    "print(env.render(mode='ainsi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run env.py\n",
    "%run rl-helpers.py\n",
    "    \n",
    "# Create environment\n",
    "obs_wrapper = BinaryGridView\n",
    "env = obs_wrapper(DeliveryDrones(n=1))\n",
    "states = env.reset()\n",
    "\n",
    "# Create agents\n",
    "agents = {drone.index: RandomAgent(env) for drone in env.drones}\n",
    "#my_agent = NeuralNetworkAgent(env, gamma=0.98, epsilon_start=1, epsilon_decay=0.99, epsilon_end=0.01)\n",
    "#my_agent = ReplayMemoryAgent(env, gamma=0.98, epsilon_start=1, epsilon_decay=0.99, epsilon_end=0.01, memory_size=10000, batch_size=64)\n",
    "#my_agent = DQNAgent(env, gamma=0.995, epsilon_start=1, epsilon_decay=0.995, epsilon_end=0.01, memory_size=10000, batch_size=64, target_update_interval=5)\n",
    "my_agent = MyDQNAgent(env, conv_sizes=[16, 16, 32], fc_sizes=[128], gamma=0.98, epsilon_start=1, epsilon_decay=0.995, epsilon_end=0.01, memory_size=10000, batch_size=32, target_update_interval=5)\n",
    "my_drone = env.drones[0]\n",
    "agents[my_drone.index] = my_agent\n",
    "\n",
    "# Train for a few steps, plot results\n",
    "trainer = MultiAgentTrainer(env, agents, seed=0)\n",
    "my_agent.is_greedy = False\n",
    "trainer.train(50000)\n",
    "plot_rolling_rewards(trainer.rewards_log, subset=range(1, 5))\n",
    "plt.plot(my_agent.epsilons)\n",
    "\n",
    "# Test agents\n",
    "my_agent.is_greedy = True\n",
    "rewards_log = test_agents(env, agents, n_steps=1000, seed=0)\n",
    "plot_cumulative_rewards(rewards_log, subset=range(1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphics\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# Drone, pickup, dropoff, collision ~Â simple geometric shapes\n",
    "# https://image.freepik.com/free-vector/simple-geometric-shapes-background_1168-371.jpg\n",
    "# Animation: slightly growing/shrinking to simulate up/down movement with shade\n",
    "\n",
    "# Desired output\n",
    "# https://img.deszone.net/2018/05/simple-geometric-shapes-free-vector-pattern4.jpg\n",
    "# https://as1.ftcdn.net/jpg/01/72/82/18/500_F_172821814_Oyl3cNYBcigDpeCzehbAQghLxJILrZA5.jpg\n",
    "\n",
    "# Other ideas\n",
    "# Drones leave a fading trace"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
