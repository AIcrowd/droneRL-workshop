{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create environment and agents\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%run env.py\n",
    "%run rl-helpers.py\n",
    "\n",
    "# Create environment:\n",
    "#   (Q-table) CompassQTable, CompassChargeQTable, LidarCompassQTable, LidarCompassChargeQTable\n",
    "#   (Grid)    WindowedGridView\n",
    "env = WindowedGridView(DeliveryDrones(), radius=3)\n",
    "\n",
    "# Create agent\n",
    "\"\"\"Q-learning agent\n",
    "my_agent = QLearningAgent(env, gamma=0.99, alpha=0.1, epsilon_start=1, epsilon_decay=0.99, epsilon_end=0.01)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"DQN with dense Q-network\n",
    "my_agent = DQNAgent(\n",
    "    env, DenseQNetworkFactory(env, hidden_layers=[256, 256]),\n",
    "    gamma=0.95, epsilon_start=0.5, epsilon_decay=0.8, epsilon_end=0.01, memory_size=10000, batch_size=64, target_update_interval=5)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"DQN with conv. Q-network\"\"\"\n",
    "my_agent = DQNAgent(\n",
    "    env, ConvQNetworkFactory(env, conv_layers=[\n",
    "        {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "        {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "        {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "        {'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "        {'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "        {'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1}\n",
    "    ], dense_layers=[256]),\n",
    "    gamma=0.95, epsilon_start=1, epsilon_decay=0.99, epsilon_end=0.01, memory_size=10000, batch_size=64, target_update_interval=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup custom environment parameters for training\n",
    "env.env_params.update({'n_drone': 3, 'pickup_reward': 1, 'discharge': 2})\n",
    "\n",
    "# Reset environment with those parameters\n",
    "env.reset()\n",
    "\n",
    "# Setup random opponents\n",
    "agents = {drone.index: RandomAgent(env) for drone in env.drones}\n",
    "agents[0] = my_agent\n",
    "\n",
    "# Create trainer\n",
    "trainer = MultiAgentTrainer(env, agents, reset_agents=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_agent.is_greedy = False\n",
    "\n",
    "# Train with different grids\n",
    "for _ in range(10):\n",
    "    trainer.train(2000) # Calls env.reset() -> new grid\n",
    "    \n",
    "    # Reset epsilon\n",
    "    my_agent.epsilon_start *= 0.99\n",
    "    my_agent.epsilon = my_agent.epsilon_start\n",
    "\n",
    "plot_rolling_rewards(trainer.rewards_log, subset=range(0, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect agents\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Q-learning agent\n",
    "if isinstance(my_agent, QLearningAgent):\n",
    "    # Q-table\n",
    "    print('Q-table:', my_agent.get_qtable().shape)\n",
    "    display(my_agent.get_qtable().sample(10))\n",
    "\n",
    "# For DQN-agent\n",
    "elif isinstance(my_agent, DQNAgent):\n",
    "    # Memory replay\n",
    "    my_agent.inspect_memory()\n",
    "    \n",
    "    # Q-network\n",
    "    print('Q-network:')\n",
    "    print(my_agent.qnetwork)\n",
    "    print()\n",
    "    \n",
    "# Epsilon decay\n",
    "plt.plot(my_agent.epsilons)\n",
    "plt.title('Epsilon decay')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.show()\n",
    "\n",
    "# Test with different seeds\n",
    "my_agent.is_greedy = True\n",
    "for i in range(10):\n",
    "    rewards_log = test_agents(env, agents, n_steps=1000, seed=i)\n",
    "    plot_cumulative_rewards(rewards_log, subset=range(0, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test agents\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "\n",
    "# Make sure our drone behaves greedily\n",
    "my_agent.is_greedy = True\n",
    "\n",
    "# Simulation loop\n",
    "states = env.reset()\n",
    "my_drone = env.drones[0]\n",
    "rewards = None\n",
    "\n",
    "while True:\n",
    "    # Render\n",
    "    clear_output(wait=True)\n",
    "    frame = Image.fromarray(env.render(mode='rgb_array'))\n",
    "    display(frame)\n",
    "\n",
    "    # Act\n",
    "    actions = {index: agent.act(states[index]) for index, agent in agents.items()}\n",
    "\n",
    "    # Print last rewards and next actions\n",
    "    print('Drone:', my_drone.index, 'charge: {}%'.format(my_drone.charge))\n",
    "    if hasattr(env, 'format_state'):\n",
    "        print('Current states:', env.format_state(states[my_drone.index]))\n",
    "    if hasattr(env, 'format_action'):\n",
    "        print('Next actions:', env.format_action(actions[my_drone.index]))\n",
    "    if rewards is not None:\n",
    "        print('Last rewards:', rewards[my_drone.index])\n",
    "\n",
    "    # Sleep, step, learn\n",
    "    time.sleep(1)\n",
    "    states, rewards, dones, _ = env.step(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run env.py\n",
    "%run rl-helpers.py\n",
    "\n",
    "# Create drones & environment\n",
    "env = WindowedGridView(DeliveryDrones(env_params={'n_drones': 10+1}), radius=3)\n",
    "states = env.reset()\n",
    "\n",
    "# Run drones\n",
    "for i in tqdm(range(10**6)):\n",
    "    states, rewards, dones, _  = env.step({drone.index: env.action_space.sample() for drone in env.drones})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
